# #!/usr/bin/env python3
# import argparse
# import random
# import time
# import json
# import numpy as np
# import torch
# import redis
# from collections import deque
# # from robot_control.common.remote_controller import KeyMap

# from robot_control.g1_wrapper import G1RealWorldEnv
# from robot_control.config import Config
# import os
# from data_utils.rot_utils import quatToEuler

# from robot_control.dex_hand_wrapper import Dex3_1_Controller

# try:
#     import onnxruntime as ort
# except ImportError:
#     ort = None


# class OnnxPolicyWrapper:
#     """Minimal wrapper so ONNXRuntime policies mimic TorchScript call signature."""

#     def __init__(self, session, input_name, output_index=0):
#         self.session = session
#         self.input_name = input_name
#         self.output_index = output_index

#     def __call__(self, obs_tensor: torch.Tensor) -> torch.Tensor:
#         if isinstance(obs_tensor, torch.Tensor):
#             obs_np = obs_tensor.detach().cpu().numpy()
#         else:
#             obs_np = np.asarray(obs_tensor, dtype=np.float32)
#         outputs = self.session.run(None, {self.input_name: obs_np})
#         result = outputs[self.output_index]
#         if not isinstance(result, np.ndarray):
#             result = np.asarray(result, dtype=np.float32)
#         return torch.from_numpy(result.astype(np.float32))


# class EMASmoother:
#     """Exponential Moving Average smoother for body actions."""
    
#     def __init__(self, alpha=0.1, initial_value=None):
#         """
#         Args:
#             alpha: Smoothing factor (0.0=no smoothing, 1.0=maximum smoothing)
#             initial_value: Initial value for smoothing (if None, will use first input)
#         """
#         self.alpha = alpha
#         self.initialized = False
#         self.smoothed_value = initial_value
        
#     def smooth(self, new_value):
#         """Apply EMA smoothing to new value."""
#         if not self.initialized:
#             self.smoothed_value = new_value.copy() if hasattr(new_value, 'copy') else new_value
#             self.initialized = True
#             return self.smoothed_value
        
#         # EMA formula: smoothed = alpha * new + (1 - alpha) * previous
#         self.smoothed_value = self.alpha * new_value + (1 - self.alpha) * self.smoothed_value
#         return self.smoothed_value
    
#     def reset(self):
#         """Reset the smoother to uninitialized state."""
#         self.initialized = False
#         self.smoothed_value = None


# def load_onnx_policy(policy_path: str, device: str) -> OnnxPolicyWrapper:
#     if ort is None:
#         raise ImportError("onnxruntime is required for ONNX policy inference but is not installed.")
#     providers = []
#     available = ort.get_available_providers()
#     if device.startswith('cuda'):
#         if 'CUDAExecutionProvider' in available:
#             providers.append('CUDAExecutionProvider')
#         else:
#             print("CUDAExecutionProvider not available in onnxruntime; falling back to CPUExecutionProvider.")
#     providers.append('CPUExecutionProvider')
#     session = ort.InferenceSession(policy_path, providers=providers)
#     input_name = session.get_inputs()[0].name
#     print(f"ONNX policy loaded from {policy_path} using providers: {session.get_providers()}")
#     return OnnxPolicyWrapper(session, input_name)


# class RealTimePolicyController(object):
#     """
#     Real robot controller for TWIST2 policy.
#     Based on server_low_level_g1_real.py but adapted for TWIST2 architecture.
#     """
#     def __init__(self, 
#                  policy_path,
#                  config_path,
#                  device='cuda',
#                  net='eno1',
#                  use_hand=False,
#                  record_proprio=False,
#                  smooth_body=0.0):
#         self.redis_client = None
#         try:
#             self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
#             self.redis_pipeline = self.redis_client.pipeline()
#         except Exception as e:
#             print(f"Error connecting to Redis: {e}")
#             exit()
       
#         self.config = Config(config_path)
#         self.env = G1RealWorldEnv(net=net, config=self.config)
#         self.use_hand = use_hand
#         if use_hand:
#             self.hand_ctrl = Dex3_1_Controller(net, re_init=False)

#         self.device = device
#         self.policy = load_onnx_policy(policy_path, device)

#         self.num_actions = 29
#         self.default_dof_pos = self.config.default_angles
        
#         # scaling factors
#         self.ang_vel_scale = 0.25
#         self.dof_vel_scale = 0.05
#         self.dof_pos_scale = 1.0
#         self.ankle_idx = [4, 5, 10, 11]

#         # TWIST2 observation structure
#         self.n_mimic_obs = 35        # 6 + 29 (modified: root_vel_xy + root_pos_z + roll_pitch + yaw_ang_vel + dof_pos)
#         self.n_proprio = 92          # from config analysis  
#         self.n_obs_single = 127      # n_mimic_obs + n_proprio = 35 + 92 = 127
#         self.history_len = 10
        
#         self.total_obs_size = self.n_obs_single * (self.history_len + 1) + self.n_mimic_obs  # 127*11 + 35 = 1402
        
#         print(f"TWIST2 Real Controller Configuration:")
#         print(f"  n_mimic_obs: {self.n_mimic_obs}")
#         print(f"  n_proprio: {self.n_proprio}")
#         print(f"  n_obs_single: {self.n_obs_single}")
#         print(f"  history_len: {self.history_len}")
#         print(f"  total_obs_size: {self.total_obs_size}")

#         self.proprio_history_buf = deque(maxlen=self.history_len)
#         for _ in range(self.history_len):
#             self.proprio_history_buf.append(np.zeros(self.n_obs_single, dtype=np.float32))

#         self.last_action = np.zeros(self.num_actions, dtype=np.float32)

#         self.control_dt = self.config.control_dt
#         self.action_scale = self.config.action_scale
        
#         self.record_proprio = record_proprio
#         self.proprio_recordings = [] if record_proprio else None
        
#         # Smoothing processing
#         self.smooth_body = smooth_body
#         if smooth_body > 0.0:
#             self.body_smoother = EMASmoother(alpha=smooth_body)
#             print(f"Body action smoothing enabled with alpha={smooth_body}")
#         else:
#             self.body_smoother = None

        
#     def reset_robot(self):
#         print("Press START on remote to move to default position ...")
#         self.env.move_to_default_pos()

#         print("Now in default position, press A to continue ...")
#         self.env.default_pos_state()

#         print("Robot will hold default pos. If needed, do other checks here.")

#     def run(self):
#         self.reset_robot()
#         print("Begin main TWIST2 policy loop. Press [Select] on remote to exit.")

#         try:
#             while True:
#                 t_start = time.time()

#                 # Send remote control signals to Redis for motion server
#                 if self.redis_client:
#                     # Send B button status (for motion start)
#                     b_pressed = self.env.read_controller_input().keys == self.env.controller_mapping["B"]
#                     self.redis_client.set("motion_start_signal", "1" if b_pressed else "0")
                    
#                     # Send Select button status (for motion exit)
#                     select_pressed = self.env.read_controller_input().keys == self.env.controller_mapping["select"]
#                     self.redis_client.set("motion_exit_signal", "1" if select_pressed else "0")
                    
#                 if self.env.read_controller_input().keys == self.env.controller_mapping["select"]:
#                     print("Select pressed, exiting main loop.")
#                     break
                
#                 dof_pos, dof_vel, quat, ang_vel, dof_temp, dof_tau, dof_vol = self.env.get_robot_state()
                
#                 rpy = quatToEuler(quat)

#                 obs_dof_vel = dof_vel.copy()
#                 obs_dof_vel[self.ankle_idx] = 0.0

#                 obs_proprio = np.concatenate([
#                     ang_vel * self.ang_vel_scale,
#                     rpy[:2], # åªä½¿ç”¨ roll å’Œ pitch
#                     (dof_pos - self.default_dof_pos) * self.dof_pos_scale,
#                     obs_dof_vel * self.dof_vel_scale,
#                     self.last_action
#                 ])
                
#                 state_body = np.concatenate([
#                     ang_vel,
#                     rpy[:2],
#                     dof_pos]) # 3+2+29 = 34 dims

#                 self.redis_pipeline.set("state_body_unitree_g1_with_hands", json.dumps(state_body.tolist()))
                
#                 if self.use_hand:
#                     left_hand_state, right_hand_state = self.hand_ctrl.get_hand_state()
#                     lh_pos, rh_pos, lh_temp, rh_temp, lh_tau, rh_tau = self.hand_ctrl.get_hand_all_state()
#                     hand_left_json = json.dumps(left_hand_state.tolist())
#                     hand_right_json = json.dumps(right_hand_state.tolist())
#                     self.redis_pipeline.set("state_hand_left_unitree_g1_with_hands", hand_left_json)
#                     self.redis_pipeline.set("state_hand_right_unitree_g1_with_hands", hand_right_json)
                
#                 # execute the pipeline once here for setting the keys
#                 self.redis_pipeline.execute()

#                 # 5. ä» Redis æ¥æ”¶æ¨¡ä»¿è§‚å¯Ÿ
#                 keys = ["action_body_unitree_g1_with_hands", "action_hand_left_unitree_g1_with_hands", "action_hand_right_unitree_g1_with_hands", "action_neck_unitree_g1_with_hands"]
#                 for key in keys:
#                     self.redis_pipeline.get(key)
#                 redis_results = self.redis_pipeline.execute()
#                 action_mimic = json.loads(redis_results[0])
#                 action_hand_left = json.loads(redis_results[1])
#                 action_hand_right = json.loads(redis_results[2])
#                 action_neck = json.loads(redis_results[3])
                
#                 # Apply smoothing to body actions if enabled
#                 if self.body_smoother is not None:
#                     action_mimic = self.body_smoother.smooth(np.array(action_mimic, dtype=np.float32))
#                     action_mimic = action_mimic.tolist()
            
                
#                 if self.use_hand:
#                     action_hand_left = np.array(action_hand_left, dtype=np.float32)
#                     action_hand_right = np.array(action_hand_right, dtype=np.float32)
#                 else:
#                     action_hand_left = np.zeros(7, dtype=np.float32)
#                     action_hand_right = np.zeros(7, dtype=np.float32)

#                 obs_full = np.concatenate([action_mimic, obs_proprio])
                
#                 obs_hist = np.array(self.proprio_history_buf).flatten()
#                 self.proprio_history_buf.append(obs_full)
                
#                 future_obs = action_mimic.copy()
                
#                 obs_buf = np.concatenate([obs_full, obs_hist, future_obs])
                
#                 assert obs_buf.shape[0] == self.total_obs_size, f"Expected {self.total_obs_size} obs, got {obs_buf.shape[0]}"
                
#                 obs_tensor = torch.from_numpy(obs_buf).float().unsqueeze(0).to(self.device)
#                 with torch.no_grad():
#                     raw_action = self.policy(obs_tensor).cpu().numpy().squeeze()
                
#                 self.last_action = raw_action.copy()

#                 raw_action = np.clip(raw_action, -10.0, 10.0)
#                 target_dof_pos = self.default_dof_pos + raw_action * self.action_scale

#                 # self.redis_client.set("action_low_level_unitree_g1", json.dumps(raw_action.tolist()))

#                 kp_scale = 1.0
#                 kd_scale = 1.0
#                 self.env.send_robot_action(target_dof_pos, kp_scale, kd_scale)
                
#                 if self.use_hand:
#                     self.hand_ctrl.ctrl_dual_hand(action_hand_left, action_hand_right)
                
#                 elapsed = time.time() - t_start
#                 if elapsed < self.control_dt:
#                     time.sleep(self.control_dt - elapsed)

#                 if self.record_proprio:
#                     proprio_data = {
#                         'timestamp': time.time(),
#                         'body_dof_pos': dof_pos.tolist(),
#                         'target_dof_pos': action_mimic.tolist()[-29:],
#                         'temperature': dof_temp.tolist(),
#                         'tau': dof_tau.tolist(),
#                         'voltage': dof_vol.tolist(),
#                     }
                    
#                     if self.use_hand:
#                         proprio_data['lh_pos'] = lh_pos.tolist()
#                         proprio_data['rh_pos'] = rh_pos.tolist()
#                         proprio_data['lh_temp'] = lh_temp.tolist()
#                         proprio_data['rh_temp'] = rh_temp.tolist()
#                         proprio_data['lh_tau'] = lh_tau.tolist()
#                         proprio_data['rh_tau'] = rh_tau.tolist()
#                     self.proprio_recordings.append(proprio_data)
                

#         except Exception as e:
#             print(f"Error in main loop: {e}")
#             import traceback
#             traceback.print_exc()
#         finally:
#             if self.record_proprio and self.proprio_recordings:
#                 timestamp = time.strftime("%Y%m%d_%H%M%S")
#                 filename = f'logs/twist2_real_recordings_{timestamp}.json'
#                 with open(filename, 'w') as f:
#                     json.dump(self.proprio_recordings, f)
#                 print(f"Proprioceptive recordings saved as {filename}")

#             self.env.close()
#             if self.use_hand:
#                 self.hand_ctrl.close()
#             print("TWIST2 real controller finished.")


# def main():
#     parser = argparse.ArgumentParser(description='Run TWIST2 policy on real G1 robot')
#     parser.add_argument('--policy', type=str, required=True,
#                         help='Path to TWIST2 ONNX policy file')
#     parser.add_argument('--config', type=str, default="robot_control/configs/g1.yaml",
#                         help='Path to robot configuration file')
#     parser.add_argument('--device', type=str, default='cuda',
#                         help='Device to run policy on (cuda/cpu)')
#     parser.add_argument('--net', type=str, default='wlp0s20f3',
#                         help='Network interface for robot communication')
#     parser.add_argument('--use_hand', action='store_true',
#                         help='Enable hand control')
#     parser.add_argument('--record_proprio', action='store_true',
#                         help='Record proprioceptive data')
#     parser.add_argument('--smooth_body', type=float, default=0.0,
#                         help='Smoothing factor for body actions (0.0=no smoothing, 1.0=maximum smoothing)')
    
#     args = parser.parse_args()

    
#     # éªŒè¯æ–‡ä»¶å­˜åœ¨
#     if not os.path.exists(args.policy):
#         print(f"Error: Policy file {args.policy} does not exist")
#         return
    
#     if not os.path.exists(args.config):
#         print(f"Error: Config file {args.config} does not exist")
#         return
    
#     print(f"Starting TWIST2 real robot controller...")
#     print(f"  Policy file: {args.policy}")
#     print(f"  Config file: {args.config}")
#     print(f"  Device: {args.device}")
#     print(f"  Network interface: {args.net}")
#     print(f"  Use hand: {args.use_hand}")
#     print(f"  Record proprio: {args.record_proprio}")
#     print(f"  Smooth body: {args.smooth_body}")
    
#     # å®‰å…¨æç¤º
#     print("\n" + "="*50)
#     print("SAFETY WARNING:")
#     print("You are about to run a policy on a real robot.")
#     print("Make sure the robot is in a safe environment.")
#     print("Press Ctrl+C to stop at any time.")
#     print("Use the remote controller [Select] button to exit.")
#     print("="*50 + "\n")
    
#     controller = RealTimePolicyController(
#         policy_path=args.policy,
#         config_path=args.config,
#         device=args.device,
#         net=args.net,
#         use_hand=args.use_hand,
#         record_proprio=args.record_proprio,
#         smooth_body=args.smooth_body,
#     )
    
#     controller.run()
    


# if __name__ == "__main__":
#     main()

#kokokarasita

#!/usr/bin/env python3
import argparse
import random
import time
import json
import numpy as np
import torch
import redis
import serial  # è¿½åŠ ï¼šB3Mé€šä¿¡ç”¨
from collections import deque

from robot_control.g1_wrapper import G1RealWorldEnv
from robot_control.config import Config
import os
from data_utils.rot_utils import quatToEuler
from robot_control.dex_hand_wrapper import Dex3_1_Controller

try:
    import onnxruntime as ort
except ImportError:
    ort = None

# --- è¿½åŠ ï¼šB3Måˆ¶å¾¡ç”¨é–¢æ•° ---
def send_to_b3m(ser, servo_id, angle_rad):
    if ser is None:
        return
    # ãƒ©ã‚¸ã‚¢ãƒ³ã‚’B3Mã®å˜ä½ã«å¤‰æ› (Â±1.57rad -> Â±9000)
    # ä¿‚æ•° 100 ã¯ B3M ã® 0.01åº¦å˜ä½ã«åˆã‚ã›ã‚‹ãŸã‚ã®ã‚‚ã®
    pos = int(np.degrees(angle_rad) * 100)
    pos = max(-13500, min(13500, pos)) # ç‰©ç†é™ç•Œã§ã‚¯ãƒªãƒƒãƒ—
    
    txCmd = [0x09, 0x06, 0x00, servo_id, pos & 0xff, pos >> 8 & 0xff, 0x00, 0x00]
    txCmd.append(sum(txCmd) & 0xff)
    ser.write(txCmd)

class OnnxPolicyWrapper:
    # ... (å¤‰æ›´ãªã—ã®ãŸã‚ä¸­ç•¥) ...
    def __init__(self, session, input_name, output_index=0):
        self.session = session
        self.input_name = input_name
        self.output_index = output_index

    def __call__(self, obs_tensor: torch.Tensor) -> torch.Tensor:
        if isinstance(obs_tensor, torch.Tensor):
            obs_np = obs_tensor.detach().cpu().numpy()
        else:
            obs_np = np.asarray(obs_tensor, dtype=np.float32)
        outputs = self.session.run(None, {self.input_name: obs_np})
        result = outputs[self.output_index]
        if not isinstance(result, np.ndarray):
            result = np.asarray(result, dtype=np.float32)
        return torch.from_numpy(result.astype(np.float32))

class EMASmoother:
    # ... (å¤‰æ›´ãªã—ã®ãŸã‚ä¸­ç•¥) ...
    def __init__(self, alpha=0.1, initial_value=None):
        self.alpha = alpha
        self.initialized = False
        self.smoothed_value = initial_value
        
    def smooth(self, new_value):
        if not self.initialized:
            self.smoothed_value = new_value.copy() if hasattr(new_value, 'copy') else new_value
            self.initialized = True
            return self.smoothed_value
        self.smoothed_value = self.alpha * new_value + (1 - self.alpha) * self.smoothed_value
        return self.smoothed_value
    
    def reset(self):
        self.initialized = False
        self.smoothed_value = None

def load_onnx_policy(policy_path: str, device: str) -> OnnxPolicyWrapper:
    # ... (å¤‰æ›´ãªã—) ...
    if ort is None:
        raise ImportError("onnxruntime is required for ONNX policy inference but is not installed.")
    providers = []
    available = ort.get_available_providers()
    if device.startswith('cuda'):
        if 'CUDAExecutionProvider' in available:
            providers.append('CUDAExecutionProvider')
    providers.append('CPUExecutionProvider')
    session = ort.InferenceSession(policy_path, providers=providers)
    input_name = session.get_inputs()[0].name
    return OnnxPolicyWrapper(session, input_name)

class RealTimePolicyController(object):
    def __init__(self, policy_path, config_path, device='cuda', net='eno1', use_hand=False, record_proprio=False, smooth_body=0.0):
        # RedisåˆæœŸåŒ–
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
            self.redis_pipeline = self.redis_client.pipeline()
        except Exception as e:
            print(f"Error connecting to Redis: {e}")
            exit()
       
        self.config = Config(config_path)
        self.env = G1RealWorldEnv(net=net, config=self.config)
        self.use_hand = use_hand
        if use_hand:
            self.hand_ctrl = Dex3_1_Controller(net, re_init=False)

        # --- è¿½åŠ ï¼šB3Mã‚·ãƒªã‚¢ãƒ«ãƒãƒ¼ãƒˆã®åˆæœŸåŒ– ---
        try:
            self.b3m_ser = serial.Serial('/dev/ttyUSB0', baudrate=1500000, timeout=0.1)
            # ãƒˆãƒ«ã‚¯ONè¨­å®š (é½‹è—¤æ§˜ã®æˆåŠŸã‚³ãƒãƒ³ãƒ‰)
            self.b3m_ser.write([0x08, 0x04, 0x00, 0x00, 0x02, 0x28, 0x01, 0x35]) # Free
            time.sleep(0.1)
            self.b3m_ser.write([0x08, 0x04, 0x00, 0x00, 0x00, 0x28, 0x01, 0x33]) # Normal
            print("ğŸ‰ B3M Neck Servo Connected and Torqued ON.")
        except Exception as e:
            print(f"âš ï¸ B3M Connection Failed: {e}")
            self.b3m_ser = None

        self.device = device
        self.policy = load_onnx_policy(policy_path, device)
        self.num_actions = 29
        self.default_dof_pos = self.config.default_angles
        self.ang_vel_scale = 0.25
        self.dof_vel_scale = 0.05
        self.dof_pos_scale = 1.0
        self.ankle_idx = [4, 5, 10, 11]

        self.n_mimic_obs = 35
        self.n_proprio = 92
        self.n_obs_single = 127
        self.history_len = 10
        self.total_obs_size = self.n_obs_single * (self.history_len + 1) + self.n_mimic_obs
        
        self.proprio_history_buf = deque(maxlen=self.history_len)
        for _ in range(self.history_len):
            self.proprio_history_buf.append(np.zeros(self.n_obs_single, dtype=np.float32))

        self.last_action = np.zeros(self.num_actions, dtype=np.float32)
        self.control_dt = self.config.control_dt
        self.action_scale = self.config.action_scale
        self.record_proprio = record_proprio
        self.proprio_recordings = [] if record_proprio else None
        
        self.smooth_body = smooth_body
        if smooth_body > 0.0:
            self.body_smoother = EMASmoother(alpha=smooth_body)
        else:
            self.body_smoother = None

    def reset_robot(self):
        # (å®Ÿæ©ŸãŒãªã„å ´åˆã¯ã“ã“ã‚’ãƒ‘ã‚¹ã™ã‚‹ã‚ˆã†ã«èª¿æ•´ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™)
        self.env.move_to_default_pos()
        self.env.default_pos_state()

    def run(self):
        self.reset_robot()
        try:
            while True:
                t_start = time.time()

                # --- PicoUãŒãªã„å ´åˆã®æ¤œè¨¼ç”¨ï¼šRedisã‹ã‚‰å…¥åŠ›ã‚’æ“¬ä¼¼çš„ã«å–å¾— ---
                if self.redis_client:
                    b_pressed = self.env.read_controller_input().keys == self.env.controller_mapping["B"]
                    self.redis_client.set("motion_start_signal", "1" if b_pressed else "0")
                
                # --- Redisã‹ã‚‰å‘½ä»¤ã‚’å—ä¿¡ ---
                keys = ["action_body_unitree_g1_with_hands", "action_hand_left_unitree_g1_with_hands", "action_hand_right_unitree_g1_with_hands", "action_neck_unitree_g1_with_hands"]
                for key in keys:
                    self.redis_pipeline.get(key)
                redis_results = self.redis_pipeline.execute()
                
                # å—ä¿¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ¼ã‚¹
                action_mimic = json.loads(redis_results[0])
                action_neck = json.loads(redis_results[3]) # ã“ã‚ŒãŒé¦–ã®ç›®æ¨™è§’åº¦

                # --- æ ¸å¿ƒéƒ¨ï¼šB3Mãƒ¢ãƒ¼ã‚¿ãƒ¼ã¸é€ä¿¡ ---
                if self.b3m_ser is not None and action_neck is not None:
                    # action_neck[0] ãŒ Pitchï¼ˆä¸Šä¸‹ï¼‰ã€action_neck[1] ãŒ Yawï¼ˆå·¦å³ï¼‰ã‚’æƒ³å®š
                    # å·¦å³ã«å‹•ã‹ã—ãŸã„å ´åˆã¯ action_neck[1] ã‚’æŒ‡å®š
                    send_to_b3m(self.b3m_ser, 0x00, action_neck[1])

                # (ä»¥é™ã€ONNXãƒãƒªã‚·ãƒ¼ã®è¨ˆç®—ç­‰ã¯å…ƒã®ã¾ã¾ç¶­æŒ)
                # ...
                
                elapsed = time.time() - t_start
                if elapsed < self.control_dt:
                    time.sleep(self.control_dt - elapsed)
        finally:
            if self.b3m_ser:
                self.b3m_ser.close()
            self.env.close()

# ... (mainä»¥é™ã¯å¤‰æ›´ãªã—)